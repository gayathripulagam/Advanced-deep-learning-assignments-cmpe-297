{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceiver.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjS4qNVdn8KIlPCYArLyci"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP9xaXp123eR"
      },
      "source": [
        "# **Perceiver for image classification**\n",
        "\n",
        "Using Perceiver architecture to classify CIFAR 10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV5mxwKTw_Mn",
        "outputId": "3e0d0ac4-5c31-43e2-fd56-a508b8809e7a"
      },
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjlRkBO26gzl"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwpg_NJj6bzs"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg741VB27I-U"
      },
      "source": [
        "Downloading CIFAR-10 dataset from Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brpMjTOto_Ws"
      },
      "source": [
        "**Using only a subset of the data (5000 images as opposed to 50000 images as shown in the Keras.io example), as training the model on the entire dataset takes 10+ hours per epoch, as shown in the attached screenshot below -**\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqmkvbGM_91R",
        "outputId": "f62752bb-e536-49aa-80bd-f584546797fb"
      },
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train[:5000]\n",
        "y_train = y_train[:5000]\n",
        "x_test = x_test[:500]\n",
        "y_test = y_test[:500]\n",
        "\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "x_train shape: (5000, 32, 32, 3) - y_train shape: (5000, 1)\n",
            "x_test shape: (500, 32, 32, 3) - y_test shape: (500, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqYlIhVYAqNt"
      },
      "source": [
        "### Defining the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0uq8vc0AmxA",
        "outputId": "bf11b48b-f772-4a63-c48e-f1a3f7ba3d87"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64 \n",
        "num_epochs = 5 #reducing the number of epochs to accelerate the training time\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  \n",
        "patch_size = 2  \n",
        "num_patches = (image_size // patch_size) ** 2  \n",
        "latent_dim = 256 \n",
        "projection_dim = 256  \n",
        "num_heads = 8  \n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "] \n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2  \n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]  \n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 1024\n",
            "Elements per patch (3 channels): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 1024 X 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0WPJZtGAzwc"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-LAU_CNAwmd"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0gh-_R0BEjw"
      },
      "source": [
        "### Feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJ4XlfJA2S5"
      },
      "source": [
        "def create_ffn(hidden_units, dropout_rate):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = keras.Sequential(ffn_layers)\n",
        "    return ffn\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n60bnqBKcK"
      },
      "source": [
        "### Patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WawyDsCTBHXR"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xlldiN-BRAe"
      },
      "source": [
        "### Patch encoding layer to linearly transform a patch by projecting it to a vector of a lower dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPcXbUC3BNws"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xk33eL8BiKY"
      },
      "source": [
        "### Defining the Perceiver model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmBunSnBBfMT"
      },
      "source": [
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n",
        "):\n",
        "\n",
        "    inputs = {\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n",
        "    }\n",
        "\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    # Create query tensor: [1, latent_dim, projection_dim].\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    # Create key tensor: [batch_size, data_dim, projection_dim].\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    # Create value tensor: [batch_size, data_dim, projection_dim].\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "    # Skip connection 1.\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    # Apply layer norm.\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    # Apply Feedforward network.\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    # Skip connection 2.\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVqj52lcBuXk"
      },
      "source": [
        "def create_transformer_module(\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "):\n",
        "\n",
        "    # input_shape: [1, latent_dim, projection_dim]\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "\n",
        "    x0 = inputs\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        # Apply layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        # Create a multi-head self-attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        # Apply layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # Apply Feedforward network.\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        # Skip connection 2.\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBG3S7P-B3nM"
      },
      "source": [
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create latent array.\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Create patching module.\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # Create patch encoder.\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "        # Create cross-attenion module.\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create Transformer module.\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create global average pooling layer.\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "        # Create a classification head.\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Augment data.\n",
        "        augmented = data_augmentation(inputs)\n",
        "        # Create patches.\n",
        "        patches = self.patcher(augmented)\n",
        "        # Encode patches.\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        # Prepare cross-attention inputs.\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        # Apply the cross-attention and the Transformer modules iteratively.\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Apply cross-attention from the latent array to the data array.\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            # Apply self-attention Transformer to the latent array.\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            # Set the latent array of the next iteration.\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        # Generate logits.\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGuIZx0rB8DP"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B4hVuxZB7Xp"
      },
      "source": [
        "def run_experiment(model):\n",
        "\n",
        "    # Create LAMB optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.LAMB(\n",
        "        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=3\n",
        "    )\n",
        "\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJQGSPbMB_bv",
        "outputId": "3397676d-f3f8-4c1c-cacf-245470d0e164"
      },
      "source": [
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "71/71 [==============================] - 5328s 75s/step - loss: 2.2254 - acc: 0.1824 - top5-acc: 0.7131 - val_loss: 2.0640 - val_acc: 0.2340 - val_top5-acc: 0.7640\n",
            "Epoch 2/5\n",
            "71/71 [==============================] - 5289s 74s/step - loss: 2.1119 - acc: 0.2138 - top5-acc: 0.7604 - val_loss: 1.9550 - val_acc: 0.2900 - val_top5-acc: 0.8100\n",
            "Epoch 3/5\n",
            "71/71 [==============================] - 5195s 73s/step - loss: 2.0597 - acc: 0.2351 - top5-acc: 0.7769 - val_loss: 1.9303 - val_acc: 0.2800 - val_top5-acc: 0.8240\n",
            "Epoch 4/5\n",
            "71/71 [==============================] - 5194s 73s/step - loss: 2.0175 - acc: 0.2482 - top5-acc: 0.8004 - val_loss: 1.9066 - val_acc: 0.2420 - val_top5-acc: 0.8220\n",
            "Epoch 5/5\n",
            "48/71 [===================>..........] - ETA: 27:26 - loss: 1.9977 - acc: 0.2617 - top5-acc: 0.8070"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICiVAVQm-DRO"
      },
      "source": [
        "The colab's runtime disconnected in the last epoch after running for 7+ hours. In the info above, we can see that the train accuracy increased with each epoch and both the training loss and the validation loss decreased with each epoch. The validation accuracy would also increase if the entire dataset is used for training and the model is trained for more iterations"
      ]
    }
  ]
}